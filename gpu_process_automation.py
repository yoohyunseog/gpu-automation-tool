import psutil
import torch
import numpy as np
import time
import threading
import json
import os
from datetime import datetime
import subprocess
import win32gui
import win32process
import win32api
import win32con
import msvcrt
import sys

class GPUProcessAutomation:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.gpu_processes = {}
        self.process_config = {}
        self.load_config()
        
    def load_config(self):
        """GPU í”„ë¡œì„¸ìŠ¤ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        config_file = "gpu_process_config.json"
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                self.process_config = json.load(f)
        else:
            # ê¸°ë³¸ ì„¤ì • ìƒì„±
            self.process_config = {
                "gpu_processes": {
                    "chrome.exe": {"priority": "high", "gpu_memory": 1024, "mode": "normal", "gpu_id": 0},
                    "firefox.exe": {"priority": "high", "gpu_memory": 1024, "mode": "normal", "gpu_id": 0},
                    "code.exe": {"priority": "medium", "gpu_memory": 512, "mode": "normal", "gpu_id": 0},
                    "python.exe": {"priority": "high", "gpu_memory": 2048, "mode": "maximum", "gpu_id": 0},
                    "notepad.exe": {"priority": "low", "gpu_memory": 256, "mode": "normal", "gpu_id": 0}
                },
                "gpu_settings": {
                    "power_mode": "prefer_maximum_performance",
                    "texture_quality": "high_quality",
                    "shader_cache": "enabled"
                }
            }
            self.save_config()
    
    def save_config(self):
        """ì„¤ì • íŒŒì¼ ì €ì¥"""
        with open("gpu_process_config.json", 'w', encoding='utf-8') as f:
            json.dump(self.process_config, f, indent=2, ensure_ascii=False)
    
    def get_process_list(self):
        """í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ëª©ë¡ ë°˜í™˜"""
        processes = []
        for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):
            try:
                processes.append({
                    'pid': proc.info['pid'],
                    'name': proc.info['name'],
                    'cpu_percent': proc.info['cpu_percent'],
                    'memory_percent': proc.info['memory_percent']
                })
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        return processes
    
    def set_process_gpu_priority(self, process_name, priority="high"):
        """í”„ë¡œì„¸ìŠ¤ì˜ GPU ìš°ì„ ìˆœìœ„ ì„¤ì •"""
        try:
            # NVIDIA GPU ì„¤ì • (NVIDIA Profile Inspector API ì‚¬ìš©)
            if priority == "high":
                self.set_nvidia_gpu_priority(process_name, "prefer_maximum_performance")
            elif priority == "medium":
                self.set_nvidia_gpu_priority(process_name, "adaptive")
            else:
                self.set_nvidia_gpu_priority(process_name, "prefer_consistent_performance")
                
            print(f"âœ… {process_name}ì˜ GPU ìš°ì„ ìˆœìœ„ë¥¼ {priority}ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            print(f"âŒ {process_name} GPU ìš°ì„ ìˆœìœ„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def set_nvidia_gpu_priority(self, process_name, power_mode):
        """NVIDIA GPU ì „ë ¥ ëª¨ë“œ ì„¤ì •"""
        try:
            # NVIDIA-SMI ëª…ë ¹ì–´ë¡œ ì „ë ¥ ëª¨ë“œ ì„¤ì •
            cmd = f'nvidia-smi -pm 1'  # ì „ë ¥ ê´€ë¦¬ í™œì„±í™”
            subprocess.run(cmd, shell=True, check=True)
            
            # í”„ë¡œì„¸ìŠ¤ë³„ GPU ì„¤ì • (ì‹¤ì œë¡œëŠ” NVIDIA Profile Inspector í•„ìš”)
            print(f"ğŸ”§ {process_name}ì— ëŒ€í•´ {power_mode} ëª¨ë“œ ì ìš©")
            
        except subprocess.CalledProcessError:
            print("âš ï¸ NVIDIA-SMI ëª…ë ¹ì–´ ì‹¤í–‰ ì‹¤íŒ¨ (NVIDIA ë“œë¼ì´ë²„ í™•ì¸ í•„ìš”)")
    
    def create_gpu_workload(self, process_name, gpu_memory_mb=1024):
        """GPU ì‘ì—… ë¶€í•˜ ìƒì„± (ì‹¤ì œ GPU ì‚¬ìš© ìœ ë„) - ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ í• ë‹¹"""
        try:
            # GPU ë©”ëª¨ë¦¬ í• ë‹¹ ë° ê³„ì‚° ì‘ì—… - ë” í° í…ì„œ ì‚¬ìš©
            memory_size = gpu_memory_mb * 1024 * 1024 // 4  # float32 ê¸°ì¤€
            
            print(f"ğŸš€ {process_name}ì— ëŒ€í•´ {gpu_memory_mb}MB GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì¤‘...")
            
            # ì—¬ëŸ¬ ê°œì˜ í° í…ì„œ ìƒì„±ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ ìµœëŒ€ í™œìš©
            tensors = []
            for i in range(5):  # 5ê°œì˜ í° í…ì„œ ìƒì„±
                tensor = torch.randn(memory_size // 5, device=self.device)
                tensors.append(tensor)
            
            # GPUì—ì„œ ì§€ì†ì ì¸ ê³„ì‚° ì‘ì—… ìˆ˜í–‰
            def continuous_gpu_work():
                iteration = 0
                while True:
                    try:
                        # ESC í‚¤ í™•ì¸
                        if self.check_esc_key():
                            print(f"\nâ¹ï¸ {process_name} GPU ì‘ì—… ì¤‘ì§€ (ESC í‚¤ ê°ì§€)")
                            break
                        
                                                    # ë³µì¡í•œ í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ GPU ë¶€í•˜ ì¦ê°€
                            for i, tensor in enumerate(tensors):
                                try:
                                    # í…ì„œ ì°¨ì› í™•ì¸ ë° ì¡°ì •
                                    if tensor.dim() == 1:
                                        # 1ì°¨ì› í…ì„œë¥¼ 2ì°¨ì›ìœ¼ë¡œ í™•ì¥
                                        tensor = tensor.unsqueeze(0)
                                    
                                    # í–‰ë ¬ ê³±ì…ˆ (2ì°¨ì› í…ì„œë§Œ)
                                    if tensor.dim() >= 2:
                                        tensor = torch.matmul(tensor, tensor.T)
                                    
                                    # í™œì„±í™” í•¨ìˆ˜ ì ìš©
                                    tensor = torch.relu(tensor)
                                    
                                    # ì •ê·œí™” (ì°¨ì› í™•ì¸)
                                    if tensor.dim() > 0:
                                        tensor = torch.nn.functional.normalize(tensor, dim=0)
                                    
                                    # ì»¨ë³¼ë£¨ì…˜ ì—°ì‚° ì‹œë®¬ë ˆì´ì…˜ (ì•ˆì „í•œ ì°¨ì› ì²˜ë¦¬)
                                    if tensor.dim() == 1:
                                        tensor = tensor.unsqueeze(0)
                                    if tensor.dim() >= 2:
                                        # ì»¨ë³¼ë£¨ì…˜ì„ ìœ„í•œ ì°¨ì› ì¡°ì •
                                        if tensor.dim() == 2:
                                            tensor = tensor.unsqueeze(0)  # (batch, channels, length)
                                        
                                        # ì»¨ë³¼ë£¨ì…˜ ì»¤ë„ í¬ê¸° ì¡°ì •
                                        kernel_size = min(3, tensor.size(-1))
                                        if kernel_size > 0:
                                            conv_kernel = torch.randn(1, tensor.size(1), kernel_size, device=self.device)
                                            tensor = torch.conv1d(tensor, conv_kernel, padding=kernel_size//2)
                                    
                                    tensors[i] = tensor
                                    
                                except Exception as e:
                                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ í…ì„œ ì¬ìƒì„±
                                    print(f"âš ï¸ í…ì„œ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}, í…ì„œ ì¬ìƒì„±")
                                    tensor_size = len(tensors[i])
                                    tensors[i] = torch.randn(tensor_size, device=self.device)
                        
                        # GPU ë™ê¸°í™”
                        torch.cuda.synchronize()
                        
                        iteration += 1
                        if iteration % 100 == 0:
                            print(f"ğŸ”„ {process_name} GPU ì‘ì—… ì§„í–‰ ì¤‘... (ë°˜ë³µ {iteration}) - ESCë¡œ ì¤‘ì§€")
                        
                        time.sleep(0.01)  # 10ms ëŒ€ê¸°
                        
                    except Exception as e:
                        print(f"âŒ GPU ì‘ì—… ì˜¤ë¥˜: {e}")
                        break
            
            # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ GPU ì‘ì—… ì‹¤í–‰
            gpu_thread = threading.Thread(target=continuous_gpu_work, daemon=True)
            gpu_thread.start()
            
            self.gpu_processes[process_name] = {
                'tensors': tensors,
                'thread': gpu_thread,
                'memory_mb': gpu_memory_mb,
                'start_time': datetime.now(),
                'iterations': 0
            }
            
            print(f"âœ… {process_name}ì— ëŒ€í•´ {gpu_memory_mb}MB GPU ì‘ì—… ë¶€í•˜ ìƒì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ GPU ì‘ì—… ë¶€í•˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def create_maximum_gpu_workload(self, process_name):
        """ìµœëŒ€ GPU ìì› í• ë‹¹ (GPU ë©”ëª¨ë¦¬ì˜ 90% ì‚¬ìš©)"""
        try:
            # GPU ë©”ëª¨ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            gpu_memory_info = torch.cuda.get_device_properties(0)
            total_memory = gpu_memory_info.total_memory // (1024 * 1024)  # MB ë‹¨ìœ„
            available_memory = int(total_memory * 0.9)  # 90% ì‚¬ìš©
            
            print(f"ğŸ”¥ {process_name}ì— ëŒ€í•´ ìµœëŒ€ GPU ìì› í• ë‹¹ ì‹œì‘...")
            print(f"ğŸ“Š ì´ GPU ë©”ëª¨ë¦¬: {total_memory}MB, í• ë‹¹ ì˜ˆì •: {available_memory}MB")
            
            # ì—¬ëŸ¬ ê°œì˜ í° í…ì„œë¡œ GPU ë©”ëª¨ë¦¬ ìµœëŒ€ í™œìš©
            tensors = []
            tensor_count = 10  # 10ê°œì˜ í° í…ì„œ ìƒì„±
            
            for i in range(tensor_count):
                tensor_size = available_memory * 1024 * 1024 // (tensor_count * 4)  # float32 ê¸°ì¤€
                tensor = torch.randn(tensor_size, device=self.device)
                tensors.append(tensor)
                print(f"ğŸ“¦ í…ì„œ {i+1}/{tensor_count} ìƒì„± ì™„ë£Œ ({tensor_size:,} ìš”ì†Œ)")
            
                # ê³ ê°•ë„ GPU ê³„ì‚° ì‘ì—…
                def intensive_gpu_work():
                    iteration = 0
                    while True:
                        try:
                            # ESC í‚¤ í™•ì¸
                            if self.check_esc_key():
                                print(f"\nâ¹ï¸ {process_name} ê³ ê°•ë„ GPU ì‘ì—… ì¤‘ì§€ (ESC í‚¤ ê°ì§€)")
                                break
                            
                            # ë³µì¡í•œ ë”¥ëŸ¬ë‹ ì—°ì‚° ì‹œë®¬ë ˆì´ì…˜
                            for i, tensor in enumerate(tensors):
                                try:
                                    # í…ì„œ ì°¨ì› í™•ì¸ ë° ì¡°ì •
                                    if tensor.dim() == 1:
                                        tensor = tensor.unsqueeze(0)
                                    
                                    # 1. í–‰ë ¬ ê³±ì…ˆ (2ì°¨ì› ì´ìƒë§Œ)
                                    if tensor.dim() >= 2:
                                        tensor = torch.matmul(tensor, tensor.T)
                                    
                                    # 2. ì»¨ë³¼ë£¨ì…˜ ì—°ì‚° (ì•ˆì „í•œ ì°¨ì› ì²˜ë¦¬)
                                    if tensor.dim() == 1:
                                        tensor = tensor.unsqueeze(0)
                                    if tensor.dim() >= 2:
                                        if tensor.dim() == 2:
                                            tensor = tensor.unsqueeze(0)
                                        
                                        # ì»¨ë³¼ë£¨ì…˜ ì»¤ë„ í¬ê¸° ì¡°ì •
                                        kernel_size = min(3, tensor.size(-1))
                                        if kernel_size > 0:
                                            conv_kernel = torch.randn(1, tensor.size(1), kernel_size, device=self.device)
                                            tensor = torch.conv1d(tensor, conv_kernel, padding=kernel_size//2)
                                    
                                    # 3. í™œì„±í™” í•¨ìˆ˜ë“¤
                                    tensor = torch.relu(tensor)
                                    tensor = torch.sigmoid(tensor)
                                    tensor = torch.tanh(tensor)
                                    
                                    # 4. ì •ê·œí™” (ì°¨ì› í™•ì¸)
                                    if tensor.dim() > 1:
                                        tensor = torch.nn.functional.normalize(tensor, dim=1)
                                    
                                    # 5. ë“œë¡­ì•„ì›ƒ ì‹œë®¬ë ˆì´ì…˜
                                    mask = torch.rand_like(tensor) > 0.1
                                    tensor = tensor * mask
                                    
                                    # 6. ë°°ì¹˜ ì •ê·œí™” ì‹œë®¬ë ˆì´ì…˜ (ì°¨ì› í™•ì¸)
                                    if tensor.dim() > 1:
                                        mean = tensor.mean(dim=1, keepdim=True)
                                        var = tensor.var(dim=1, keepdim=True, unbiased=False)
                                        tensor = (tensor - mean) / torch.sqrt(var + 1e-5)
                                    
                                    tensors[i] = tensor
                                    
                                except Exception as e:
                                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ í…ì„œ ì¬ìƒì„±
                                    print(f"âš ï¸ ê³ ê°•ë„ í…ì„œ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}, í…ì„œ ì¬ìƒì„±")
                                    tensor_size = len(tensors[i])
                                    tensors[i] = torch.randn(tensor_size, device=self.device)
                            
                            # GPU ë™ê¸°í™”
                            torch.cuda.synchronize()
                            
                            iteration += 1
                            if iteration % 50 == 0:
                                # GPU ì‚¬ìš©ë¥  í™•ì¸
                                gpu_util, mem_used, mem_total = self.monitor_gpu_usage()
                                print(f"ğŸ”¥ {process_name} ê³ ê°•ë„ GPU ì‘ì—… ì§„í–‰ ì¤‘... (ë°˜ë³µ {iteration}, GPU ì‚¬ìš©ë¥ : {gpu_util}%) - ESCë¡œ ì¤‘ì§€")
                            
                            time.sleep(0.005)  # 5ms ëŒ€ê¸° (ë” ë¹ ë¥¸ ë°˜ë³µ)
                            
                        except Exception as e:
                            print(f"âŒ ê³ ê°•ë„ GPU ì‘ì—… ì˜¤ë¥˜: {e}")
                            break
            
            # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ê³ ê°•ë„ GPU ì‘ì—… ì‹¤í–‰
            gpu_thread = threading.Thread(target=intensive_gpu_work, daemon=True)
            gpu_thread.start()
            
            self.gpu_processes[process_name] = {
                'tensors': tensors,
                'thread': gpu_thread,
                'memory_mb': available_memory,
                'start_time': datetime.now(),
                'iterations': 0,
                'intensive_mode': True
            }
            
            print(f"ğŸ”¥ {process_name}ì— ëŒ€í•´ ìµœëŒ€ GPU ìì› í• ë‹¹ ì™„ë£Œ ({available_memory}MB)")
            return True
            
        except Exception as e:
            print(f"âŒ ìµœëŒ€ GPU ìì› í• ë‹¹ ì‹¤íŒ¨: {e}")
            return False
    
    def create_specific_gpu_workload(self, process_name, gpu_id=0):
        """íŠ¹ì • GPUì— ì‘ì—… í• ë‹¹ (GPU 0, 1, 2 ë“±)"""
        try:
            gpu_count = torch.cuda.device_count()
            if gpu_id >= gpu_count:
                print(f"âŒ GPU {gpu_id}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ GPU: 0-{gpu_count-1}")
                return False
            
            # GPU ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            gpu_props = torch.cuda.get_device_properties(gpu_id)
            total_memory = gpu_props.total_memory // (1024 * 1024)  # MB ë‹¨ìœ„
            available_memory = int(total_memory * 0.8)  # 80% ì‚¬ìš©
            
            print(f"ğŸ¯ {process_name}ì— ëŒ€í•´ GPU {gpu_id} ì „ìš© ì‘ì—… í• ë‹¹ ì‹œì‘...")
            print(f"ğŸ“Š GPU {gpu_id} ì •ë³´: {gpu_props.name}")
            print(f"ğŸ“Š ì´ ë©”ëª¨ë¦¬: {total_memory}MB, í• ë‹¹ ì˜ˆì •: {available_memory}MB")
            
            # ì§€ì •ëœ GPUì— í…ì„œ ìƒì„±
            with torch.cuda.device(gpu_id):
                tensors = []
                tensor_count = 8  # 8ê°œì˜ í° í…ì„œ ìƒì„±
                
                for i in range(tensor_count):
                    tensor_size = available_memory * 1024 * 1024 // (tensor_count * 4)  # float32 ê¸°ì¤€
                    tensor = torch.randn(tensor_size, device=f'cuda:{gpu_id}')
                    tensors.append(tensor)
                    print(f"ğŸ“¦ GPU {gpu_id} í…ì„œ {i+1}/{tensor_count} ìƒì„± ì™„ë£Œ ({tensor_size:,} ìš”ì†Œ)")
                
                # ì§€ì •ëœ GPUì—ì„œ ê³ ê°•ë„ ì‘ì—… ìˆ˜í–‰
                def specific_gpu_work():
                    iteration = 0
                    while True:
                        try:
                            # ESC í‚¤ í™•ì¸
                            if self.check_esc_key():
                                print(f"\nâ¹ï¸ {process_name} GPU {gpu_id} ì‘ì—… ì¤‘ì§€ (ESC í‚¤ ê°ì§€)")
                                break
                            
                            # ë³µì¡í•œ ë”¥ëŸ¬ë‹ ì—°ì‚° ì‹œë®¬ë ˆì´ì…˜
                            for i, tensor in enumerate(tensors):
                                try:
                                    # í…ì„œ ì°¨ì› í™•ì¸ ë° ì¡°ì •
                                    if tensor.dim() == 1:
                                        tensor = tensor.unsqueeze(0)
                                    
                                    # 1. í–‰ë ¬ ê³±ì…ˆ (2ì°¨ì› ì´ìƒë§Œ)
                                    if tensor.dim() >= 2:
                                        tensor = torch.matmul(tensor, tensor.T)
                                    
                                    # 2. ì»¨ë³¼ë£¨ì…˜ ì—°ì‚° (ì•ˆì „í•œ ì°¨ì› ì²˜ë¦¬)
                                    if tensor.dim() == 1:
                                        tensor = tensor.unsqueeze(0)
                                    if tensor.dim() >= 2:
                                        if tensor.dim() == 2:
                                            tensor = tensor.unsqueeze(0)
                                        
                                        # ì»¨ë³¼ë£¨ì…˜ ì»¤ë„ í¬ê¸° ì¡°ì •
                                        kernel_size = min(3, tensor.size(-1))
                                        if kernel_size > 0:
                                            conv_kernel = torch.randn(1, tensor.size(1), kernel_size, device=f'cuda:{gpu_id}')
                                            tensor = torch.conv1d(tensor, conv_kernel, padding=kernel_size//2)
                                    
                                    # 3. í™œì„±í™” í•¨ìˆ˜ë“¤
                                    tensor = torch.relu(tensor)
                                    tensor = torch.sigmoid(tensor)
                                    tensor = torch.tanh(tensor)
                                    
                                    # 4. ì •ê·œí™” (ì°¨ì› í™•ì¸)
                                    if tensor.dim() > 1:
                                        tensor = torch.nn.functional.normalize(tensor, dim=1)
                                    
                                    # 5. ë“œë¡­ì•„ì›ƒ ì‹œë®¬ë ˆì´ì…˜
                                    mask = torch.rand_like(tensor) > 0.1
                                    tensor = tensor * mask
                                    
                                    # 6. ë°°ì¹˜ ì •ê·œí™” ì‹œë®¬ë ˆì´ì…˜ (ì°¨ì› í™•ì¸)
                                    if tensor.dim() > 1:
                                        mean = tensor.mean(dim=1, keepdim=True)
                                        var = tensor.var(dim=1, keepdim=True, unbiased=False)
                                        tensor = (tensor - mean) / torch.sqrt(var + 1e-5)
                                    
                                    tensors[i] = tensor
                                    
                                except Exception as e:
                                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ í…ì„œ ì¬ìƒì„±
                                    print(f"âš ï¸ GPU {gpu_id} í…ì„œ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}, í…ì„œ ì¬ìƒì„±")
                                    tensor_size = len(tensors[i])
                                    tensors[i] = torch.randn(tensor_size, device=f'cuda:{gpu_id}')
                            
                            # GPU ë™ê¸°í™”
                            torch.cuda.synchronize(gpu_id)
                            
                            iteration += 1
                            if iteration % 50 == 0:
                                # GPU ì‚¬ìš©ë¥  í™•ì¸
                                gpu_util, mem_used, mem_total = self.monitor_specific_gpu_usage(gpu_id)
                                print(f"ğŸ¯ GPU {gpu_id} ê³ ê°•ë„ ì‘ì—… ì§„í–‰ ì¤‘... (ë°˜ë³µ {iteration}, GPU ì‚¬ìš©ë¥ : {gpu_util}%) - ESCë¡œ ì¤‘ì§€")
                            
                            time.sleep(0.005)  # 5ms ëŒ€ê¸°
                            
                        except Exception as e:
                            print(f"âŒ GPU {gpu_id} ì‘ì—… ì˜¤ë¥˜: {e}")
                            break
                
                # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ GPU ì‘ì—… ì‹¤í–‰
                gpu_thread = threading.Thread(target=specific_gpu_work, daemon=True)
                gpu_thread.start()
                
                self.gpu_processes[process_name] = {
                    'tensors': tensors,
                    'thread': gpu_thread,
                    'gpu_id': gpu_id,
                    'memory_mb': available_memory,
                    'start_time': datetime.now(),
                    'iterations': 0,
                    'specific_gpu_mode': True
                }
                
                print(f"âœ… {process_name}ì— ëŒ€í•´ GPU {gpu_id} ì „ìš© ì‘ì—… í• ë‹¹ ì™„ë£Œ ({available_memory}MB)")
                return True
                
        except Exception as e:
            print(f"âŒ GPU {gpu_id} ì‘ì—… í• ë‹¹ ì‹¤íŒ¨: {e}")
            return False
    
    def create_multi_gpu_workload(self, process_name):
        """ë©€í‹° GPU ì§€ì› (ì—¬ëŸ¬ GPUì— ì‘ì—… ë¶„ì‚°)"""
        try:
            gpu_count = torch.cuda.device_count()
            if gpu_count < 2:
                print("âš ï¸ ë©€í‹° GPUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¨ì¼ GPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                return self.create_maximum_gpu_workload(process_name)
            
            print(f"ğŸš€ {process_name}ì— ëŒ€í•´ {gpu_count}ê°œ GPUì— ì‘ì—… ë¶„ì‚° ì‹œì‘...")
            
            gpu_tensors = {}
            gpu_threads = {}
            
            for gpu_id in range(gpu_count):
                # ê° GPUë³„ë¡œ í…ì„œ ìƒì„±
                with torch.cuda.device(gpu_id):
                    tensors = []
                    for i in range(5):
                        tensor = torch.randn(1024 * 1024, device=f'cuda:{gpu_id}')
                        tensors.append(tensor)
                    gpu_tensors[gpu_id] = tensors
                    
                    # ê° GPUë³„ ì‘ì—… ìŠ¤ë ˆë“œ
                    def gpu_work(gpu_id):
                        iteration = 0
                        while True:
                            try:
                                # ESC í‚¤ í™•ì¸
                                if self.check_esc_key():
                                    print(f"\nâ¹ï¸ GPU {gpu_id} ì‘ì—… ì¤‘ì§€ (ESC í‚¤ ê°ì§€)")
                                    break
                                
                                for i, tensor in enumerate(gpu_tensors[gpu_id]):
                                    try:
                                        # í…ì„œ ì°¨ì› í™•ì¸ ë° ì¡°ì •
                                        if tensor.dim() == 1:
                                            tensor = tensor.unsqueeze(0)
                                        
                                        # í–‰ë ¬ ê³±ì…ˆ (2ì°¨ì› ì´ìƒë§Œ)
                                        if tensor.dim() >= 2:
                                            tensor = torch.matmul(tensor, tensor.T)
                                        
                                        # í™œì„±í™” í•¨ìˆ˜ ì ìš©
                                        tensor = torch.relu(tensor)
                                        
                                        # ì •ê·œí™” (ì°¨ì› í™•ì¸)
                                        if tensor.dim() > 0:
                                            tensor = torch.nn.functional.normalize(tensor, dim=0)
                                        
                                        gpu_tensors[gpu_id][i] = tensor
                                        
                                    except Exception as e:
                                        # ì˜¤ë¥˜ ë°œìƒ ì‹œ í…ì„œ ì¬ìƒì„±
                                        print(f"âš ï¸ ë©€í‹° GPU {gpu_id} í…ì„œ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}, í…ì„œ ì¬ìƒì„±")
                                        tensor_size = len(gpu_tensors[gpu_id][i])
                                        gpu_tensors[gpu_id][i] = torch.randn(tensor_size, device=f'cuda:{gpu_id}')
                                
                                torch.cuda.synchronize(gpu_id)
                                iteration += 1
                                
                                if iteration % 100 == 0:
                                    print(f"ğŸ”„ GPU {gpu_id} ì‘ì—… ì§„í–‰ ì¤‘... (ë°˜ë³µ {iteration}) - ESCë¡œ ì¤‘ì§€")
                                
                                time.sleep(0.01)
                                
                            except Exception as e:
                                print(f"âŒ GPU {gpu_id} ì‘ì—… ì˜¤ë¥˜: {e}")
                                break
                    
                    thread = threading.Thread(target=gpu_work, args=(gpu_id,), daemon=True)
                    thread.start()
                    gpu_threads[gpu_id] = thread
            
            self.gpu_processes[process_name] = {
                'gpu_tensors': gpu_tensors,
                'gpu_threads': gpu_threads,
                'gpu_count': gpu_count,
                'start_time': datetime.now(),
                'multi_gpu_mode': True
            }
            
            print(f"âœ… {process_name}ì— ëŒ€í•´ {gpu_count}ê°œ GPU ì‘ì—… ë¶„ì‚° ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë©€í‹° GPU ì‘ì—… ë¶„ì‚° ì‹¤íŒ¨: {e}")
            return False
    
    def monitor_specific_gpu_usage(self, gpu_id):
        """íŠ¹ì • GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§"""
        try:
            # NVIDIA-SMIë¡œ íŠ¹ì • GPU ì‚¬ìš©ë¥  í™•ì¸
            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', 
                                   '--format=csv,noheader,nounits', f'--id={gpu_id}'], 
                                  capture_output=True, text=True)
            
            if result.returncode == 0:
                gpu_info = result.stdout.strip().split(', ')
                gpu_util = int(gpu_info[0])
                memory_used = int(gpu_info[1])
                memory_total = int(gpu_info[2])
                
                print(f"ğŸ“Š GPU {gpu_id} ì‚¬ìš©ë¥ : {gpu_util}%, ë©”ëª¨ë¦¬: {memory_used}/{memory_total} MB")
                return gpu_util, memory_used, memory_total
            else:
                print(f"âš ï¸ GPU {gpu_id} ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None, None, None
                
        except Exception as e:
            print(f"âŒ GPU {gpu_id} ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
            return None, None, None
    
    def list_available_gpus(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ GPU ëª©ë¡ í‘œì‹œ"""
        try:
            gpu_count = torch.cuda.device_count()
            print(f"\nğŸ® ì‚¬ìš© ê°€ëŠ¥í•œ GPU ëª©ë¡ ({gpu_count}ê°œ):")
            
            for gpu_id in range(gpu_count):
                gpu_props = torch.cuda.get_device_properties(gpu_id)
                total_memory = gpu_props.total_memory // (1024 * 1024)  # MB ë‹¨ìœ„
                print(f"  GPU {gpu_id}: {gpu_props.name} ({total_memory}MB)")
            
            return gpu_count
            
        except Exception as e:
            print(f"âŒ GPU ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0
    
    def monitor_gpu_usage(self):
        """GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§"""
        try:
            # NVIDIA-SMIë¡œ GPU ì‚¬ìš©ë¥  í™•ì¸
            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', 
                                   '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            
            if result.returncode == 0:
                gpu_info = result.stdout.strip().split(', ')
                gpu_util = int(gpu_info[0])
                memory_used = int(gpu_info[1])
                memory_total = int(gpu_info[2])
                
                print(f"ğŸ“Š GPU ì‚¬ìš©ë¥ : {gpu_util}%, ë©”ëª¨ë¦¬: {memory_used}/{memory_total} MB")
                return gpu_util, memory_used, memory_total
            else:
                print("âš ï¸ GPU ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None, None, None
                
        except Exception as e:
            print(f"âŒ GPU ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
            return None, None, None
    
    def optimize_process_for_gpu(self, process_name, mode="normal", gpu_id=0):
        """í”„ë¡œì„¸ìŠ¤ë¥¼ GPU ìµœì í™”"""
        config = self.process_config.get("gpu_processes", {}).get(process_name, {})
        priority = config.get("priority", "medium")
        gpu_memory = config.get("gpu_memory", 512)
        
        print(f"ğŸ”§ {process_name} GPU ìµœì í™” ì‹œì‘... (ëª¨ë“œ: {mode}, GPU: {gpu_id})")
        
        # 1. GPU ìš°ì„ ìˆœìœ„ ì„¤ì •
        self.set_process_gpu_priority(process_name, priority)
        
        # 2. GPU ì‘ì—… ë¶€í•˜ ìƒì„± (ëª¨ë“œì— ë”°ë¼)
        if mode == "maximum":
            self.create_maximum_gpu_workload(process_name)
        elif mode == "multi_gpu":
            self.create_multi_gpu_workload(process_name)
        elif mode == "gpu_0":
            self.create_specific_gpu_workload(process_name, gpu_id)
        else:
            self.create_gpu_workload(process_name, gpu_memory)
        
        # 3. í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ ì¡°ì •
        self.set_process_priority(process_name, priority)
        
        print(f"âœ… {process_name} GPU ìµœì í™” ì™„ë£Œ")
    
    def set_process_priority(self, process_name, priority):
        """í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ ì„¤ì •"""
        try:
            for proc in psutil.process_iter(['pid', 'name']):
                if proc.info['name'] == process_name:
                    if priority == "high":
                        proc.nice(psutil.HIGH_PRIORITY_CLASS)
                    elif priority == "medium":
                        proc.nice(psutil.NORMAL_PRIORITY_CLASS)
                    else:
                        proc.nice(psutil.BELOW_NORMAL_PRIORITY_CLASS)
                    
                    print(f"ğŸ“ˆ {process_name} (PID: {proc.info['pid']}) ìš°ì„ ìˆœìœ„ë¥¼ {priority}ë¡œ ì„¤ì •")
                    break
        except Exception as e:
            print(f"âŒ í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def add_process_to_config(self, process_name, priority="medium", gpu_memory=512, mode="normal", gpu_id=0):
        """ìƒˆ í”„ë¡œì„¸ìŠ¤ë¥¼ ì„¤ì •ì— ì¶”ê°€"""
        self.process_config["gpu_processes"][process_name] = {
            "priority": priority,
            "gpu_memory": gpu_memory,
            "mode": mode,
            "gpu_id": gpu_id
        }
        self.save_config()
        print(f"â• {process_name}ì„ ì„¤ì •ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. (ëª¨ë“œ: {mode}, GPU: {gpu_id})")
    
    def remove_process_from_config(self, process_name):
        """í”„ë¡œì„¸ìŠ¤ë¥¼ ì„¤ì •ì—ì„œ ì œê±°"""
        if process_name in self.process_config["gpu_processes"]:
            del self.process_config["gpu_processes"][process_name]
            self.save_config()
            print(f"â– {process_name}ì„ ì„¤ì •ì—ì„œ ì œê±°í–ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ {process_name}ì´ ì„¤ì •ì— ì—†ìŠµë‹ˆë‹¤.")
    
    def check_esc_key(self):
        """ESC í‚¤ ì…ë ¥ í™•ì¸ (ë¹„ë™ê¸°)"""
        try:
            if msvcrt.kbhit():
                key = msvcrt.getch()
                if key == b'\x1b':  # ESC í‚¤
                    return True
        except:
            pass
        return False
    
    def start_automation(self):
        """ìë™í™” ì‹œì‘"""
        print("ğŸš€ GPU í”„ë¡œì„¸ìŠ¤ ìë™í™” ì‹œì‘...")
        print("ğŸ’¡ ESC í‚¤ë¥¼ ëˆ„ë¥´ë©´ ë©”ì¸ ë©”ë‰´ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.")
        
        while True:
            try:
                # ESC í‚¤ í™•ì¸
                if self.check_esc_key():
                    print("\nâ¹ï¸ ESC í‚¤ ê°ì§€! ë©”ì¸ ë©”ë‰´ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤...")
                    break
                
                # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                processes = self.get_process_list()
                
                # ì„¤ì •ëœ í”„ë¡œì„¸ìŠ¤ë“¤ í™•ì¸ ë° ìµœì í™”
                for proc in processes:
                    process_name = proc['name']
                    if process_name in self.process_config["gpu_processes"]:
                        if process_name not in self.gpu_processes:
                            # ìƒˆë¡œìš´ í”„ë¡œì„¸ìŠ¤ ë°œê²¬, GPU ìµœì í™” ì‹œì‘
                            config = self.process_config["gpu_processes"][process_name]
                            mode = config.get("mode", "normal")
                            gpu_id = config.get("gpu_id", 0)
                            
                            if mode == "gpu_0":
                                self.optimize_process_for_gpu(process_name, "gpu_0", gpu_id)
                            elif mode == "maximum":
                                self.optimize_process_for_gpu(process_name, "maximum")
                            elif mode == "multi_gpu":
                                self.optimize_process_for_gpu(process_name, "multi_gpu")
                            else:
                                self.optimize_process_for_gpu(process_name, "normal")
                
                # GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
                self.monitor_gpu_usage()
                
                # 1ì´ˆ ëŒ€ê¸° (ESC í‚¤ ê°ì§€ë¥¼ ìœ„í•´ ë” ì§§ê²Œ)
                time.sleep(1)
                
            except KeyboardInterrupt:
                print("\nâ¹ï¸ Ctrl+C ê°ì§€! ìë™í™” ì¤‘ì§€...")
                break
            except Exception as e:
                print(f"âŒ ìë™í™” ì˜¤ë¥˜: {e}")
                time.sleep(1)
    
    def show_current_status(self):
        """í˜„ì¬ ìƒíƒœ í‘œì‹œ"""
        print("\n=== GPU í”„ë¡œì„¸ìŠ¤ ìë™í™” ìƒíƒœ ===")
        
        # ì„¤ì •ëœ í”„ë¡œì„¸ìŠ¤ë“¤
        print("\nğŸ“‹ ì„¤ì •ëœ í”„ë¡œì„¸ìŠ¤ë“¤:")
        for proc_name, config in self.process_config["gpu_processes"].items():
            status = "ğŸŸ¢ í™œì„±" if proc_name in self.gpu_processes else "âšª ë¹„í™œì„±"
            mode = config.get("mode", "normal")
            gpu_id = config.get("gpu_id", 0)
            print(f"  {status} {proc_name} - ìš°ì„ ìˆœìœ„: {config['priority']}, ëª¨ë“œ: {mode}, GPU: {gpu_id}, ë©”ëª¨ë¦¬: {config['gpu_memory']}MB")
        
        # GPU ì‚¬ìš©ë¥ 
        gpu_util, mem_used, mem_total = self.monitor_gpu_usage()
        
        # í™œì„± GPU í”„ë¡œì„¸ìŠ¤ë“¤
        print("\nğŸš€ í™œì„± GPU í”„ë¡œì„¸ìŠ¤ë“¤:")
        for proc_name, info in self.gpu_processes.items():
            runtime = datetime.now() - info['start_time']
            print(f"  {proc_name} - ë©”ëª¨ë¦¬: {info['memory_mb']}MB, ì‹¤í–‰ì‹œê°„: {runtime}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    automation = GPUProcessAutomation()
    
    while True:
        print("\n=== GPU í”„ë¡œì„¸ìŠ¤ ìë™í™” ë„êµ¬ ===")
        print("1. í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ëª©ë¡ ë³´ê¸°")
        print("2. í”„ë¡œì„¸ìŠ¤ GPU ìµœì í™” ì‹œì‘")
        print("3. ìµœëŒ€ GPU ìì› í• ë‹¹ (90% ë©”ëª¨ë¦¬ ì‚¬ìš©)")
        print("4. ë©€í‹° GPU ì‘ì—… ë¶„ì‚°")
        print("5. GPU 0 ì „ìš© ëª¨ë“œ")
        print("6. ì‚¬ìš© ê°€ëŠ¥í•œ GPU ëª©ë¡ ë³´ê¸°")
        print("7. ìë™í™” ëª¨ë“œ ì‹œì‘")
        print("8. í”„ë¡œì„¸ìŠ¤ ì„¤ì • ì¶”ê°€/ì œê±°")
        print("9. í˜„ì¬ ìƒíƒœ í™•ì¸")
        print("10. ì¢…ë£Œ")
        print("\nğŸ’¡ ëª¨ë“  ì‹¤í–‰ ìƒíƒœì—ì„œ ESC í‚¤ë¥¼ ëˆ„ë¥´ë©´ ë©”ì¸ ë©”ë‰´ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤!")
        
        choice = input("\nì„ íƒí•˜ì„¸ìš” (1-10): ").strip()
        
        if choice == "1":
            processes = automation.get_process_list()
            print(f"\nğŸ“‹ í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ({len(processes)}ê°œ):")
            for proc in processes[:20]:  # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ
                print(f"  {proc['name']} (PID: {proc['pid']}) - CPU: {proc['cpu_percent']:.1f}%, ë©”ëª¨ë¦¬: {proc['memory_percent']:.1f}%")
        
        elif choice == "2":
            process_name = input("GPU ìµœì í™”í•  í”„ë¡œì„¸ìŠ¤ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if process_name:
                automation.optimize_process_for_gpu(process_name, "normal")
        
        elif choice == "3":
            process_name = input("ìµœëŒ€ GPU ìì›ì„ í• ë‹¹í•  í”„ë¡œì„¸ìŠ¤ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if process_name:
                automation.optimize_process_for_gpu(process_name, "maximum")
        
        elif choice == "4":
            process_name = input("ë©€í‹° GPUì— ì‘ì—…ì„ ë¶„ì‚°í•  í”„ë¡œì„¸ìŠ¤ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if process_name:
                automation.optimize_process_for_gpu(process_name, "multi_gpu")
        
        elif choice == "5":
            process_name = input("GPU 0ì— ì‘ì—…ì„ í• ë‹¹í•  í”„ë¡œì„¸ìŠ¤ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if process_name:
                gpu_id = input("GPU ID (ê¸°ë³¸ê°’: 0): ").strip() or "0"
                automation.optimize_process_for_gpu(process_name, "gpu_0", int(gpu_id))
        
        elif choice == "6":
            automation.list_available_gpus()
        
        elif choice == "7":
            print("ìë™í™” ëª¨ë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. Ctrl+Cë¡œ ì¤‘ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            automation.start_automation()
        
        elif choice == "8":
            print("\n1. í”„ë¡œì„¸ìŠ¤ ì¶”ê°€")
            print("2. í”„ë¡œì„¸ìŠ¤ ì œê±°")
            sub_choice = input("ì„ íƒí•˜ì„¸ìš” (1-2): ").strip()
            
            if sub_choice == "1":
                name = input("í”„ë¡œì„¸ìŠ¤ ì´ë¦„: ").strip()
                priority = input("ìš°ì„ ìˆœìœ„ (high/medium/low): ").strip() or "medium"
                memory = input("GPU ë©”ëª¨ë¦¬ (MB): ").strip() or "512"
                
                print("\nGPU ëª¨ë“œ ì„ íƒ:")
                print("1. ì¼ë°˜ ëª¨ë“œ (normal)")
                print("2. ìµœëŒ€ GPU ìì› (maximum)")
                print("3. ë©€í‹° GPU ë¶„ì‚° (multi_gpu)")
                print("4. GPU 0 ì „ìš© (gpu_0)")
                mode_choice = input("ëª¨ë“œ ì„ íƒ (1-4): ").strip() or "1"
                
                mode = "normal"
                gpu_id = 0
                
                if mode_choice == "2":
                    mode = "maximum"
                elif mode_choice == "3":
                    mode = "multi_gpu"
                elif mode_choice == "4":
                    mode = "gpu_0"
                    gpu_id_input = input("GPU ID (ê¸°ë³¸ê°’: 0): ").strip() or "0"
                    gpu_id = int(gpu_id_input)
                
                automation.add_process_to_config(name, priority, int(memory), mode, gpu_id)
            
            elif sub_choice == "2":
                name = input("ì œê±°í•  í”„ë¡œì„¸ìŠ¤ ì´ë¦„: ").strip()
                automation.remove_process_from_config(name)
        
        elif choice == "9":
            automation.show_current_status()
        
        elif choice == "10":
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 